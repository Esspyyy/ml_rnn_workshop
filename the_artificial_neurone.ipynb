{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The artificial neurone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](neurone_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram illustrates the basic structure of a single artificial neuron, often referred to as a perceptron, which is a fundamental building block of neural networks.\n",
    "\n",
    "### Explanation of the Diagram:\n",
    "1. **Inputs $(X_1, X_2, \\dots, X_n)$**:\n",
    "   - These represent the features or signals fed into the neuron. Each input has an associated **weight** ($W_1, W_2, \\dots, W_n$) that determines the importance or contribution of that input to the neuron's output.\n",
    "\n",
    "2. **Weights $(W_1, W_2, \\dots, W_n)$**:\n",
    "   - These are scalar values that multiply with the corresponding inputs. The weights are crucial as they adjust the input values, influencing how much they contribute to the final decision or prediction.\n",
    "\n",
    "3. **Bias $(b)$**:\n",
    "   - The bias is an additional parameter that allows the model to fit the data better. It helps in shifting the activation function to the left or right, which can be crucial for the learning process.\n",
    "\n",
    "4. **Summation Function $(Î£)$**:\n",
    "   - This is where all the weighted inputs are summed up along with the bias. Mathematically, this can be represented as:\n",
    "     $a = (W_1 \\cdot X_1) + (W_2 \\cdot X_2) + \\dots + (W_n \\cdot X_n) + b$\n",
    "   - Here, $a$ represents the net input to the activation function.\n",
    "\n",
    "5. **Activation Function $F(a)$**:\n",
    "   - The output of the summation function is then passed through an activation function $F(a)$. This function introduces non-linearity into the model, allowing the network to learn and model complex data patterns. Common activation functions include the Sigmoid, ReLU, and Tanh functions.\n",
    "\n",
    "6. **Output $(Y)$**:\n",
    "   - After the activation function is applied, the final output $Y$ is produced. This output can represent a decision, such as classifying an input, or in more complex models, it can be passed as input to another neuron in a multi-layer network.\n",
    "\n",
    "### Summary:\n",
    "This diagram represents a simple neural network's operation, showing how inputs are weighted, summed, and then processed through an activation function to produce an output. This process is repeated across many neurons and layers to enable the neural network to learn and make predictions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
