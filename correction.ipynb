{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0\n",
    "---\n",
    "\n",
    "**There are many ways to represent the matrix, here is one example :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N1 = np.array([0, 1, 0, 1])\n",
    "N2 = np.array([1, 0, 1, 0])\n",
    "\n",
    "W1 = np.array([[0,0,0,0],\n",
    "               [1,0,1,0],\n",
    "               [0,0,0,0],\n",
    "               [0,0,0,0]]) # complete this matrix\n",
    "\n",
    "result = N1 @ W1 # the @ operator is used for matrix multiplication\n",
    "\n",
    "print(\"Résultat après multiplication :\")\n",
    "print(result)\n",
    "\n",
    "assert np.array_equal(result, N2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "target_output = np.array([1.0, 0.0, 1.0, 0.0])\n",
    "\n",
    "predicted_output = np.array([0.9, 0.1, 0.8, 0.2])\n",
    "\n",
    "# Todo : Calculate the Mean Squared Error\n",
    "mse_loss = np.mean((target_output - predicted_output) ** 2)\n",
    "\n",
    "print(\"Mean Squared Error\", mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's try to implement our iterative gradient descent with a simple 1D function\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def function(x):\n",
    "    return x**2 + 4*x + 4\n",
    "\n",
    "# Define the gradient of the function\n",
    "def gradient(x):\n",
    "    return 2*x + 4\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "iterations = 20\n",
    "start_point = 10  # Starting point for gradient descent\n",
    "\n",
    "# Lists to store the values of x and f(x) for each iteration\n",
    "x_values = []\n",
    "f_values = []\n",
    "\n",
    "# Gradient Descent Algorithm\n",
    "x = start_point\n",
    "for i in range(iterations):\n",
    "    x_values.append(x)\n",
    "    f_values.append(function(x))\n",
    "    x = x - learning_rate * gradient(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return max(0.1 * x, x)\n",
    "\n",
    "def softmax(x):\n",
    "    return exp(x) / exp(x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "target = np.array([2.0, 3.0, 4.0, 5.0])\n",
    "\n",
    "W1 = np.array([[1.5, 1.3, 1.8, 1.1],\n",
    "              [1.5, 1.3, 1.8, 1.1],\n",
    "              [1.5, 1.3, 1.8, 1.1],\n",
    "              [1.5, 1.3, 1.8, 1.1]]) # weights for the input layer that are randomly initialized for 1 example\n",
    "\n",
    "prediction = input @ W1\n",
    "\n",
    "loss = np.mean((prediction - target) ** 2) # mean squared error\n",
    "print(\"loss\", loss)\n",
    "\n",
    "G1 = 2 * np.mean((prediction - target) * input) # gradient of the loss function\n",
    "print(\"gradient\", G1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The parameters $0.01$ can be change it is not fix** see it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo update the weights with the gradient\n",
    "\n",
    "New_W1 = W1 - 0.01 * G1\n",
    "\n",
    "prediction = New_W1 @ input\n",
    "loss = np.mean((prediction - target) ** 2)\n",
    "print(\"new loss\", loss) # should be smaller than the previous loss\n",
    "\n",
    "W1 = New_W1\n",
    "# Don't hesitate to run the code multiple times to see the loss decreasing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
