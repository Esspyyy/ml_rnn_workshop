{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We have previously gotten familiar with the concept of a **loss function**. It can help us quantify how far off our predictions are from the actual values. The next step would be to know what changes to make to our weights and biases in order to minimize the loss function. That is where **gradient descent** comes into play.\n",
    "**Gradient descent** works by iteratively adjusting the model's parameters (weights and biases) in the direction that decreases the loss function.\n",
    "\n",
    "\n",
    "Let's first simplify the problem to 1 dimension in order to make it more approachable :\n",
    "\n",
    "Minimizing the loss function means finding a local minima. *What value of $x$ gives me the lowest $y$ ?*\n",
    "\n",
    "\n",
    "Those of you familiar with calculus know we can determine the slope of the function at any given point by finding the derivative. The derivative tells us how steep the function is and in which direction it is slopingâ€”whether it's increasing or decreasing. If the slope (or derivative) is positive, it means that the function is increasing at that point, so to minimize the loss, we would need to move in the negative direction. Conversely, if the slope is negative, the function is decreasing, and we would move in the positive direction to reach the minimum. Think of it as a ball rolling down a hill.\n",
    "\n",
    "![alt text](../../Source/youtube-video-gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a neural network we are dealing with loss functions that have a lot more than one parameter.\n",
    "\n",
    "The term \"**gradient**\" refers to a generalization of the derivative to multiple dimensions. When dealing with functions of multiple variables, like a function $f(x_1, x_2, \\dots, x_n)$, the **gradient** is a vector that contains all the partial derivatives of the function with respect to each of its variables.\n",
    "\n",
    "For example, if we have a function $f(x, y)$, the gradient of $f$, denoted as $\\nabla f$, is a vector that consists of two components:\n",
    "\n",
    "\n",
    "$\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)$\n",
    "\n",
    "\n",
    "Each component tells us the rate of change of the function with respect to one of the variables while keeping the others constant.\n",
    "\n",
    "For each iteration we will update our parameters by moving them in the opposite direction of the **gradient**, scaled by a factor known as the **learning rate**. The general formula for gradient descent is:\n",
    "\n",
    "$\\theta_{t+1} = \\theta_t - \\alpha \\nabla_{\\theta} L(\\theta_t)$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\theta_t$ represents the parameter vector at iteration $t$.\n",
    "- $\\alpha$ is the **learning rate**, an arbitrary positive scalar that determines the size of the steps we take in the direction of the negative gradient.\n",
    "- $\\nabla_{\\theta} L(\\theta_t)$ is the gradient of the loss function $L(\\theta)$ with respect to the parameters $\\theta$ at iteration $t$. This gradient is a vector where each component is the partial derivative of the loss function with respect to a specific parameter in $\\theta$.\n",
    "\n",
    "![alt text](<../../Source/gradient descent.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's try to implement our iterative gradient descent with a simple 1D function\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function to minimize\n",
    "def function(x):\n",
    "    # TODO: Implement the function f(x) = x^2 + 4*x + 4\n",
    "    return ...\n",
    "\n",
    "\n",
    "# Define the gradient of the function\n",
    "def gradient(x):\n",
    "    # TODO: Implement the gradient of the above function\n",
    "    return ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualization\n",
    "plt.plot(x_values, f_values, 'bo-', label='Gradient Descent Path')\n",
    "x_range = np.linspace(-10, 10, 400)\n",
    "plt.plot(x_range, function(x_range), 'r-', label='Function f(x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent Visualization')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Take me back to the Optimization Function!](<3.1 concept_of_optimization_function.ipynb>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
